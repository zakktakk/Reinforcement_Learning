\begin{block}{Introduction}
    \begin{itemize}
        \item How do we perform such a diverse set of complex tasks?
            \begin{itemize}              
                \item Given an MDP with options, $\mdp
                    \tuple{\states,\options,\transitions,\cdot}$, can we
                    quickly learn any task (i.e. different $\rewards$)?
            \end{itemize}
        \item Most literature focuses on finding options to reach `bottlenecks',
            which are common subgoals across tasks. The objective of these
            options is to aid in early exploration.
            \begin{itemize}              
                \item A. McGovern and A. G. Barto, ``Automatic Discovery of
                    Subgoals in Reinforcement Learning using Diverse Density,''
                    in ICML, 2001
                \item I. Menache, S. Mannor, and N. Shimkin, ``Q-Cut - Dynamic
                    Discovery of Sub-Goals in Reinforcement Learning,'' in ECML,
                    2002. 
                \item \"{O}. \c{S}im\c{s}ek and A. G. Barto, ``Skill
                    characterization based on betweenness,'' in NIPS, 2008 
            \end{itemize}              
        \item Our1 Hypothesis: The key is in finding a set of composable subtasks
            spanning the space of tasks.
    \end{itemize}              
\end{block}

\vfill
\begin{block}{Motivation: The Small World Phenomenon}
    \begin{itemize}
        \item Kleinberg: ``Individuals using local information are collectively
            very effective at actually constructing short paths between two
            points in a social network.'' \\
            {\small J. Kleinberg, ``The Small-World
            Phenomenon: An Algorithmic Perspective'' in ACM Theory of Computing,
            2000}
        \item Kleinberg constructed a family of networks for which the expected
            time to deliver a message from any source to any destination was
            $( \log |\mbox{size of network}| )^2$, using the inverse power law distribution.
            \begin{itemize}              
                \item Structural properties of the network are important
            \end{itemize}              
        \item Can we do the same for learning?
            \begin{itemize}              
                \item A {\bf small-world RL domain} has the property that an
                    agent using local information (e.g. the value function) can
                    effectively reach a state of {\em maximal value}.
            \end{itemize}              
    \end{itemize}
\end{block}

\vfill
\begin{block}{Generating Options according to $P_r$}
    \begin{columns}
        \begin{column}{.39\textwidth}
            \begin{figure}[h]
                \centering
                \includegraphics[height=3in]{figures/rooms-options}
                \label{fig:rooms-options}
                \caption{Some $P_2$ Options}
            \end{figure}
        \end{column}
        \begin{column}{.49\textwidth}                
            \begin{itemize}
                \item Consider the state-interaction graph of $\mdp$. 
                \item For each state $s \in \states$, select a single $s'$
                    reachable from $s$ according to the inverse power law
                    distribution $P_r : p(s,s') \propto \|s - s'\|^{-r}$.
                \item For each $(s,s')$ pair, construct an option $o:\option$
                    with $\initset = \{s\}$, $\stopcond = \{s'\}$, and $\pi =
                    \mbox{optimal policy to reach $s'$}$.
            \end{itemize}
        \end{column}
    \end{columns}

    {\em Note: This construction adds just one additional action for each state,
    and thus does not blow up the agent's search space.}

\end{block}

\vfill
\begin{block}{Theorem: $O( (\log n)^2 )$ Decisions}
    Assume $\mdp$ to have states arranged in a $k$-dimensional lattice, with
    noisy (with parameter $\epsilon$) primitive navigation actions $\actions$,
    and rewards distributed between $[0,1]$.

    Using only the value of neighboring states, an agent with options $\options$
    generated by $P_k$, can reach a state of maximal value in $O( ( \log
    |\states| )^2)$ decisions.

    \begin{itemize}
        \item We relate the value of two states $u$ and $v$, and their lattice distance,
            $$ \log \frac{ V(v) }{ V(u) } \approx \log( \sqrt{ \frac{1 - \epsilon}{ \epsilon } } ) \|u - v\| + c,$$ 
            where $c \in [0,\frac{1}{1-\gamma}]$.
        \item Following Kleinberg's analysis, we show that using the optimal
            value function, the agent makes $O(\log |\states|)$ decisions to get
            exponentially closer to the maximal value state. 
    \end{itemize}

\end{block}
