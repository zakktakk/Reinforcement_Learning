\section{Introduction}
\label{sec:intro}

% General Introduction
In large domains, RL agents generally require a large number of samples to learn
a good policy. The options framework proposed by Sutton, Precup and Singh
\cite{SuttonPrecupSingh1998} provides extended actions for which a policy is
already learnt, reducing the complexity of the learning task, and generally
making the learning task faster.  An open question in the options framework is
discovering the options themselves.  There has been substantial work to learn
options, mainly focussed around identifying ``bottleneck'' states, either
empirically as in the work bye Stolle \cite{Stolle}, or more recently, using
graph theoretic methods like betweeness \cite{Simsek} or graph partitions
\cite{Simsek2005} explored by Simsek and Barto.

% Motivation
In this work, we propose a method for creating options motivated from a
cognitive perspective, based on the following hypothesis: we memorise many
actions, not necessarily bottleneck ones, and evolve them. Based on their
necessity in solving problems these actions are either reinforced, or gradually
forgotten. The actions could be of varying complexity, and it is intuitive to
expect that we probably learn a great deal more {\em simple} actions than
complex ones. In context of the options framework, these actions correspond to
options, and ``complex actions'' correspond to longer options.

% Our options
A desirable set of options gives the agent a set of skills which can be put
together to efficiently accomplish almost any task. From the perspective of the
state-space interaction graph, this is similar to the problem of distributed
search studied by Kleinberg \cite{Kleinberg}; adding edges to a graph such that
any node can be efficiently reached. Guided by this intuition, the method we
propose generates options using a generalisation of the inverse-square law,
along the lines of the small-world graph generation model proposed by Kleinberg.

% Summary of the results
Our results show that agents trained using our `small-world' options indeed
perform well, and converge to optimal performance quickly and with little
variance. 

