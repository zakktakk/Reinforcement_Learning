# 修論タイトル
指導教員：大橋弘忠  
東京大学大学院工学系研究科システム創成学専攻  

37-166357 山崎卓朗

## 目次
### 第1章 序論
#### 背景と目的
#### 構成

### 第2章 social learningの枠組み
#### 繰り返しゲーム
#### 進化ゲーム理論との違い
ルールベースで記憶が入ってないよ。エージェントが自分の学習を通して学習して行くよ。

#### 強化学習アルゴリズム
##### Sarsa
参考文献：http://www.hi.is.uec.ac.jp/rcb/paper/PDF/H16_sasaki.pdf
近年、強化学習の計算理論が人間をはじめとした動物の脳内に存在することを示唆する研究が行われている。そもそも、より多くの報酬を獲得するための行動を自律的、かつ探索的に学習するという強化学習エージェントの適応的な振る舞いは、動物の行動学習の最も基本的な側面であると言える。


### 第3章 ネットワーク、強化学習の基礎的な統計との関係分析
#### エージェント数(2,3,少人数、ネットワークに繋がない)、エッジ数、中心性、利得行列
#### アルゴリズム、学習率、、、学習率の平均化

### 第4章 今回の新規性部分
#### ルールベースでやってる
追従性、

### 第5章 考察

### 第6章 結論

### 謝辞

### 参考文献

問題点

* 修論の軸となる分析
* 枠組みが統一されてない

過去の経歴に依存するのか
適応度の谷を越えられるか (一回下がって上がる)
マルチエージェント学習の特徴 (進化ゲームの戦略)(行動の決め方は得られた報酬に依存する)
n人ゲームへの拡張 (公共財)
条件がダイナミックに変わる時の追従性 (途中で利得行列を変える)
