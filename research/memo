9/2
*** Collective Learning in Games through Social Networks
	概要：
		この論文ではソーシャルネットワーク上でのコニュニケーションがエージェントの学習にpositiveな影響を与えることを述べている．
		ここでは協力ゲームにのみ話を絞り，ソーシャルネットの効果を検証している

	"Maximization, learning, and economic behavior" -> 人工知能だけでなく社会現象の理解に強化学習が有用であることを述べている
	こいつらはネットワークの構造とゲームを別物として考えている
	マルチエージェントのゲームを考える．エージェントは行動決定の前に意見交換の機会をえる．エージェントはコミュニケーションを通して共通の
	ゴール＝効用の最大化を目指す．

	2章 グラフ理論に基づく学習 -> Lehrer-Wagner modelと呼ばれる
	3章 確率的な学習
	4章 強化学習ベースの学習
	5章 上記3つの手法の統合
	6章 議論，心理学実験への示唆

	Lehrer-Wagner model [Lehrer and Wanger, 1981]
		各エージェントは全てのjoint strategiesに対して行動の確率分布をもつ．この分布は主観的な(行動への)信頼度と解釈することができる．
		エージェント間の信頼関係はエージェントグラフ上のリンクの重みで表される．エージェントの信頼度はステップごとに重み付き平均わで更新される

	これ数学的な解釈のみに止まってて実験行ってないな．結論だけ下にまとめるわ．

	結論：
		本稿ではネットワーク上でのコミュニケーション，信頼の集約，強化学習の手順を踏むモデルを提案した．特定の構造を持つソーシャルネットは
		協力ゲームに好影響を与えることを発見した．また，社会的な最適解に近い行動の確率分布を持つプレイヤーは他のプレイヤーからの信頼度が高く
		，そうでない場合は低かった(当たり前)．この問題設定ではエージェントは報酬関数とネットワーク構造に関して無知である．

	Trustを実装するときはこ論文を参考にしよう．この論文ではtrustは固定なのか?

	https://www.illc.uva.nl/Research/Publications/Reports/MoL-2015-04.text.pdf
	この人のmaster thesisがこれでソーシャルネット上のゲームを題材にしているので参考にしよう．と思ったけどこれも数学だけだ


*** 社会規範学習に置ける空間ゲームの局所性の影響
	概要：
		この論文では均衡点が複数存在する空間ゲームにおいて，エージェントの位置関係や学習アルゴリズムが社会規範発生に与える影響を解析する．
		次に社会規範発生を促す要因を特定し，その影響を確認する．

	"Emergence of norms through social learning" -> 複数のエージェントが局所的に強調ゲームを繰り返す環境において，各エージェントが過去の経験から学習して環境に適用する中で社会規範が発生することを示した

	"Norm Emergence under constrained interactions in diverse societies" -> エージェントの位置関係が社会規範発生までの速度に影響することを示した．
	本論文ではQ学習とWoLF-PHCが使われている．Q学習は最も広く用いられているため採用．WoLFは2エージェントによる繰り返しゲームでナッシュ均衡に収束することが保証されているから．

	Social LearningはSenらが考えた言葉であり，「ゲームの利得行列が既知の状態で行動選択」「対戦相手を区別できないが，対戦相手のとった行動は知覚できる」という状況を表す．Senらはこれを「道路上のルールの学習」と呼んだ．-> 二つの車が道路上で対面した場合，左右どちらによって走るべきかを学習するってことらしい．本論文の利得行列では利得の最大値が4であり，上記"Norm Emergence..."では平均利得が3.5に達した時社会規範が発生したとみなしている

	hauertらは進化ゲーム理論の観点から戦略進化を行う過程でネットワーク構造が協調行動の創発に与える影響について解析した．
		-> Natureの論文でした．"Spatial structure often inhibits the evolution of cooperation in the snowdrift game", 2004
		snowdrift game＝タカハトゲー＝チキンゲーム．この論文では100*100のlaticeにエージェントを配置してる(一見ライフゲームっぽい)．
		目的は協調行動の発生理解．その典型問題として囚人のジレンマとタカハトゲームが挙げられる．囚人のジレンマは通常非協力的な状態が
		進化的に安定であるが，空間上に拡張すると協調行動が促進されることがわかっている(1992年のnature, Nowak.M.A, May R.M)．
		通常，タカハトゲームは協調行動に収束するが，空間に拡張したタカハトゲームは協調行動が減った．特に協調することのcost-to-benefit ratioが
		高い場合はそれが顕著だった．-> ここから単に空間拡張が協調行動の要因と言えるわけではないことが示唆された．
		タカハトゲームはこちらも参照(M. Tomassini, et al. "hawks and doves on small-world networks")

	マルチエージェント強化学習に関する既存の研究では2エージェントで繰り返しゲームを行うもしくは個々の利得が全エージェントの行動に依存する環境を扱っている場合が多い．本研究は3以上のエージェントが存在し，各エージェントの対戦相手が毎回変化する点でこれらの研究とは異なる
	-> 一般的でない問題設定ということ？

	だけどこれらの研究はエージェントが自律的に行動を学習するわけではないの本論文とは別の立場になる．

	実験項目
		・近傍エージェント数＆アルゴリズムごとの平均利得の推移比較
		・wolf-phcの収束解に至る推移
		・方策をheatmapで示したもの
		・社会規範が発生しなかった事例の平均利得推移
		・エージェント数の影響
		・スモールワールド性を考慮した実験
			>> 最遠のエージェント同士を確率pで結んでる．本当にスモールワールド性を満たしているのかは微妙

	今後の課題
		・スタグハントなど他のゲームでの検証

** Games on Networks(https://web.stanford.edu/~jacksonm/GamesNetworks.pdf)
	概要：
		
SARSA, ...を追加